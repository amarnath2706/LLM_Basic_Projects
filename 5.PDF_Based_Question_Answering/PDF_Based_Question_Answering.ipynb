{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465c003a-29cf-4cfe-a0c6-d36c04ae2b37",
   "metadata": {},
   "source": [
    "# QA over PDF file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a73fbf-e241-499d-a235-32a87b46ee7c",
   "metadata": {},
   "source": [
    "## Intro\n",
    "* We will create a Q&A app that can answer questions about PDF files.\n",
    "* We will use a Document Loader to load text in a format usable by an LLM, then build a retrieval-augmented generation (RAG) pipeline to answer questions, including citations from the source material.\n",
    "* **We will use a basic approach for this project. You will see more advanced ways to solve the same problem in next projects**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "groq_api_key = os.environ[\"GROQ_API_KEY\"]\n",
    "google_api_key = os.environ['GOOGLE_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ae8595e-5c07-4b02-8a79-db55fd357527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our chat completion model\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"mixtral-8x7b-32768\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99edb898-4503-429d-ad7d-fad4fc74300f",
   "metadata": {},
   "source": [
    "## Load the PDF file\n",
    "* The loader reads the PDF at the specified path into memory.\n",
    "* It then extracts text data using the pypdf package.\n",
    "* Finally, it creates a LangChain Document for each page of the PDF with the page's content and some metadata about where in the document the text came from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e1e2ab-2870-43a0-bfdf-4da12047c1d1",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following packages because they are already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "764957d8-b4b3-4555-affc-187203e85293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"./data/Be_Good.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cb64f5c-d5da-4c60-9418-a9d5407faa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Be Good - Essay by Paul Graham\n",
      "Be Good\n",
      "Be good\n",
      "April 2008(This essay is derived from a talk at the 2\n",
      "{'source': './data/Be_Good.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[0:100])\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1148577-8c06-47fb-9d48-fa9ce7f14e30",
   "metadata": {},
   "source": [
    "## RAG\n",
    "* We will use the vector database (aka. vector store) Chroma DB.\n",
    "* Using a text splitter, we will split the loaded PDF into smaller documents that can more easily fit into an LLM's context window, then load them into a vector store.\n",
    "* We can then create a retriever from the vector store for use in our RAG chain:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cfdcb7-6f73-46a9-9a28-6c1550021088",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39ae6680-d314-435f-9213-ffb66bac7b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "#from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f674aa-167a-46f1-8e5c-c7f62d237092",
   "metadata": {},
   "source": [
    "#### We will use two pre-defined chains to construct the final rag_chain:\n",
    "In this exercise we are going to use two pre-defined chains to build the final chain:\n",
    "* create_stuff_documents_chain\n",
    "* create_retrieval_chain\n",
    "* Let's learn a little bit more about these two pre-defined chains.\n",
    "\n",
    "#### create_stuff_documents_chain\n",
    "The create_stuff_documents_chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window of the LLM you are using.\n",
    "1. **Taking a List of Documents**: This function starts by receiving a group of documents that you provide.\n",
    "  \n",
    "2. **Formatting into a Prompt**: It then takes all these documents and organizes them into a specific prompt. A prompt is essentially a text setup that is used to feed information into a language model (like an LLM, or Large Language Model).\n",
    "\n",
    "3. **Passing to an LLM**: After formatting the documents into a prompt, this function sends the formatted prompt to a language model. The model will process this information to perform tasks like answering questions, generating text, etc.\n",
    "\n",
    "4. **Fit within Context Window**: The function sends all the documents at once to the LLM. However, it's important to make sure that the total length of the prompt does not exceed what the LLM can handle at one time. This limit is known as the \"context window\" of the LLM. If the prompt is too long, the model might not process it effectively.\n",
    "\n",
    "In simpler terms, think of this chain as a way of taking several pieces of text, bundling them together in a specific way, and then feeding them to an LLM that reads and uses this bundled text to do its job. Just make sure the bundle isnâ€™t too big for the LLM to handle at once!\n",
    "\n",
    "\n",
    "#### create_retrieval_chain\n",
    "The create_retrieval_chain takes in a user inquiry, which is then passed to the retriever to fetch relevant documents. Those documents (and original inputs) are then passed to an LLM to generate a response.\n",
    "1. **Receiving a User Inquiry**: This process begins when a user asks a question or makes a request.\n",
    "\n",
    "2. **Using a Retriever to Fetch Documents**: The function then uses a retriever to find documents that are relevant to the user's inquiry. This means it searches through available information to pick out parts that can help answer the question.\n",
    "\n",
    "3. **Passing Information to an LLM**: After gathering the relevant documents, both these documents and the original user inquiry are sent to an LLM.\n",
    "\n",
    "4. **Generating a Response**: The LLM processes all the information it receives to come up with an appropriate response, which is then given back to the user.\n",
    "\n",
    "In simpler terms, this chain acts like a smart assistant that first looks up information based on your question, gathers useful details, and then uses those details along with your original question to craft a helpful answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac58036b-b6bc-44d6-8d76-b6141d8a9da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This article by Paul Graham is about the importance of being good in business, not just to have a positive image but also as a strategy for success. He suggests that being good can lead to genuine growth and help a company maintain a competitive edge. He also discusses the concept of Microsoft\\'s \"don\\'t be evil\" approach as a potential elixir of corporate youth. The article is not a sanctimonious call to be good, but rather a pragmatic argument for its effectiveness.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "results = rag_chain.invoke({\"input\": \"What is this article about?\"})\n",
    "\n",
    "results[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89746b08-1ae3-4f9b-971d-449a32d9c7e7",
   "metadata": {},
   "source": [
    "* If you print the whole `results` you will see that **you get both the answer, and the context the LLM used to generate that answer**. See it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87e341ec-36f7-4703-97d5-ce71cd2ecac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is this article about?',\n",
       " 'context': [Document(metadata={'page': 10, 'source': './data/Be_Good.pdf'}, page_content=\"Be Good - Essay by Paul Graham\\nGoogle does.Most explicitly benevolent projects don't hold themselves sufficiently\\naccountable.  They act as if having good intentions were enough to\\nguarantee good effects.[3] Users dislike their\\nnew operating system so much that they're starting petitions to\\nsave the old one.  And the old one was nothing special.  The hackers\\nwithin Microsoft must know in their hearts that if the company\\nreally cared about users they'd just advise them to switch to OSX.Thanks to Trevor Blackwell, Paul\\nBuchheit, Jessica Livingston,\\nand Robert Morris for reading drafts of this.\\nPage 11\"),\n",
       "  Document(metadata={'page': 9, 'source': './data/Be_Good.pdf'}, page_content=\"You can't be buying users; that's a pyramid scheme.   But a company\\nwith rapid, genuine growth is valuable, and eventually markets learn\\nhow to value valuable things.[2] The idea of starting\\na company with benevolent aims is currently undervalued, because\\nthe kind of people who currently make that their explicit goal don't\\nusually do a very good job.It's one of the standard career paths of trustafarians to start\\nsome vaguely benevolent business.  The problem with most of them\\nis that they either have a bogus political agenda or are feebly\\nexecuted.  The trustafarians' ancestors didn't get rich by preserving\\ntheir traditional culture; maybe people in Bolivia don't want to\\neither.  And starting an organic farm, though it's at least\\nstraightforwardly benevolent, doesn't help people on the scale that\\nPage 10\"),\n",
       "  Document(metadata={'page': 3, 'source': './data/Be_Good.pdf'}, page_content='thinks of what\\nMicrosoft does to users, all the verbs that come to mind begin with\\nF.  [3] And yet it doesn\\'t seem to pay.\\nTheir stock price has been flat for years.  Back when they were\\nRobin Hood, their stock price rose like Google\\'s.  Could there be\\na connection?You can see how there would be.  When you\\'re small, you can\\'t bully\\ncustomers, so you have to charm them.  Whereas when you\\'re big you\\ncan maltreat them at will, and you tend to, because it\\'s easier\\nthan satisfying them.  You grow big by being nice, but you can stay\\nbig by being mean.You get away with it till the underlying conditions change, and\\nthen all your victims escape.  So \"Don\\'t be evil\" may be the most\\nvaluable thing Paul Buchheit made for Google, because it may turn\\nout to be an elixir of corporate youth.  I\\'m sure they find it\\nconstraining, but think how valuable it will be if it saves them\\nPage 4'),\n",
       "  Document(metadata={'page': 8, 'source': './data/Be_Good.pdf'}, page_content='it\\'s the only algorithm that works on that scale.When you write something telling people to be good,\\nyou seem to be\\nclaiming to be good yourself.  So I want to say explicitly that I\\nam not a particularly good person.  When I was a kid I was firmly\\nin the camp of bad.  The way adults used the word good, it seemed\\nto be synonymous with quiet, so I grew up very suspicious of it.You know how there are some\\npeople whose names come up in conversation\\nand everyone says \"He\\'s such a great guy?\"  People never say\\nthat about me.  The best I get is \"he means well.\"  I am not claiming\\nto be good.  At best I speak good as a second language.So I\\'m not suggesting you be good in the\\nusual sanctimonious way.\\nI\\'m suggesting it because it works.  It will work not just as a\\nstatement of \"values,\" but as a guide to strategy,\\nand even a design spec for software.  Don\\'t just not be evil.  Be\\nPage 9')],\n",
       " 'answer': 'This article by Paul Graham is about the importance of being good in business, not just to have a positive image but also as a strategy for success. He suggests that being good can lead to genuine growth and help a company maintain a competitive edge. He also discusses the concept of Microsoft\\'s \"don\\'t be evil\" approach as a potential elixir of corporate youth. The article is not a sanctimonious call to be good, but rather a pragmatic argument for its effectiveness.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7225ee-ff03-4c41-a4b5-1138ab3cfbfb",
   "metadata": {},
   "source": [
    "* Examining the values under the context further, you can see that they are documents that each contain a chunk of the ingested page content. These documents also preserve the original **metadata** from way back when you first loaded them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2153a091-c9cb-489e-b505-c9691b4cc774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 10, 'source': './data/Be_Good.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print(results[\"context\"][0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41a05cb-2700-4f40-9b39-5313f636391e",
   "metadata": {},
   "source": [
    "* This particular chunk came from page 0 in the original PDF. You can use this data to show which page in the PDF the answer came from, allowing users to quickly verify that answers are based on the source material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e1fd97-edd3-4142-9e1a-adefb0fe5c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic_llm_projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
